{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Set up & Install"
      ],
      "metadata": {
        "id": "YTiR5qv_HJLe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSSdiKnJyy14",
        "outputId": "5ed86951-3d45-4f73-8fa8-5e0b7930669d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cpu)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWIRz_rFrxk4"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXE8AUJ0GOGm"
      },
      "source": [
        "Comparing model\n",
        "- Original LSTM\n",
        "- Bidirectional LSTM\n",
        "- Unidirectional GRU\n",
        "- Custom Mamba\n",
        "- MG SMM\n",
        "- MG SMM-s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwlmwe-Usw5O"
      },
      "source": [
        "##Original LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dSoqtAwhswUH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class OriginalLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(OriginalLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        # Initialize hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1yQolRjpQG"
      },
      "source": [
        "##Bi LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "90826050"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BidirectionalLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(BidirectionalLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # batch_first=True allows input and output tensors of shape (batch_size, seq_len, features)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        # The output of a bidirectional LSTM is the concatenation of the forward and backward hidden states\n",
        "        # So the linear layer's input size is 2 * hidden_size\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        # For bidirectional LSTM, the shape is (2 * num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, 2 * hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_LAOTuYHf4H"
      },
      "source": [
        "##Original GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "665saUOhHhjH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UnidirectionalGRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard (unidirectional) Gated Recurrent Unit (GRU) model in PyTorch.\n",
        "    It processes a sequence and uses the last hidden state for a linear classification/regression layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        \"\"\"\n",
        "        Initializes the UnidirectionalGRUModel.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The number of expected features in the input $x$.\n",
        "            hidden_size (int): The number of features in the hidden state $h$.\n",
        "            num_layers (int): Number of recurrent layers.\n",
        "            output_size (int): The size of the output from the final linear layer.\n",
        "        \"\"\"\n",
        "        super(UnidirectionalGRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Changed 'bidirectional=True' to 'bidirectional=False' (or just omit it, as False is the default)\n",
        "        # batch_first=True allows input and output tensors of shape (batch_size, seq_len, features)\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
        "\n",
        "        # The output of a unidirectional GRU at each time step is just the hidden_size\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        # Initialize the hidden state\n",
        "        # For a unidirectional GRU, the shape is (num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # hn: tensor of shape (num_layers, batch_size, hidden_size)\n",
        "        out, _ = self.gru(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the *last* time step.\n",
        "        # out[:, -1, :] selects the output features for the last element in the sequence\n",
        "        # for all batches. This output is the final hidden state of the top layer.\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rT38UKH5ILS"
      },
      "source": [
        "##Custom mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cWBnm6uL5H8C"
      },
      "outputs": [],
      "source": [
        "class Custom_MambaCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Custom_MambaCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # cell input gate\n",
        "        self.W_ic = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hc = nn.Linear(hidden_size, hidden_size)\n",
        "        self.b_c = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        h_prev = hidden_state\n",
        "\n",
        "        # Hidden state\n",
        "        h_t = self.W_ic(x) + self.W_hc(h_prev) + self.b_c\n",
        "\n",
        "        return h_t\n",
        "\n",
        "class Custom_MambaModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(Custom_MambaModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm_cells = nn.ModuleList([Custom_MambaCell(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        hidden_state = [torch.zeros(batch_size, self.hidden_size).to(x.device) # Initialize with a single tensor\n",
        "                         for _ in range(self.num_layers)]\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            input_t = x[:, t, :]\n",
        "            for i in range(self.num_layers):\n",
        "                h_prev = hidden_state[i]\n",
        "\n",
        "                # Mamba part (using the Custom_MambaCell)\n",
        "                h_t = self.lstm_cells[i](input_t, h_prev) # Pass a single tensor\n",
        "                hidden_state[i] = h_t\n",
        "                input_t = h_t # Use the output of the current layer as input for the next layer\n",
        "\n",
        "        # Decode the hidden state of the last time step of the last layer\n",
        "        last_hidden_state = hidden_state[-1]\n",
        "        out = self.fc(last_hidden_state)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUQ1fP8CoOo0"
      },
      "source": [
        "##MG SMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-BMUERh7oRrj"
      },
      "outputs": [],
      "source": [
        "class MgSmmCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, gate_size):\n",
        "        super(MgSmmCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gate_size = gate_size\n",
        "\n",
        "        # linear input gate\n",
        "        self.W_A = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_B = nn.Linear(input_size, hidden_size)\n",
        "        self.W_bh = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # multiplicative input gate\n",
        "        self.W_E = nn.Linear(gate_size, gate_size)\n",
        "        self.W_bg = nn.Parameter(torch.zeros(gate_size))\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        h_prev, g_prev = hidden_state\n",
        "\n",
        "        # Hidden state\n",
        "        h_t =  self.W_A(h_prev) + self.W_B(x) +  self.W_bh\n",
        "        g_t =  self.W_E(g_prev) * torch.repeat_interleave(x, self.gate_size, dim=1)\n",
        "\n",
        "        return h_t, g_t\n",
        "\n",
        "class MgSmmModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, gate_size = 0):\n",
        "        \"\"\"\n",
        "        Initializes the CustomMambaModel.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The number of expected features in the input $x$.\n",
        "            hidden_size (int): The number of features in the hidden state $h$.\n",
        "            num_layers (int): Number of recurrent layers.\n",
        "            output_size (int): The size of the output from the final linear layer.\n",
        "            gate_size (int): The number of features in the multiplicative gate cell.\n",
        "        \"\"\"\n",
        "        super(MgSmmModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if gate_size == 0:\n",
        "            gate_size = int(hidden_size/2)\n",
        "        self.gate_size = gate_size\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.SSM_blocks = nn.ModuleList([MgSmmCell(input_size if i == 0 else hidden_size, hidden_size, gate_size) for i in range(num_layers)])\n",
        "\n",
        "        self.W_C = nn.Linear(hidden_size, output_size)\n",
        "        self.W_D = nn.Linear(input_size, output_size)\n",
        "        self.W_J = nn.Linear(gate_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        hidden_state = [(torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
        "                                torch.ones(batch_size, self.gate_size).to(x.device)\n",
        "                         ) for _ in range(self.num_layers)]\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            input_t = x[:, t, :]\n",
        "            for i in range(self.num_layers):\n",
        "                h_prev, g_prev = hidden_state[i]\n",
        "\n",
        "                # Mamba part (using the Custom_MambaCell)\n",
        "                h_t, c_t = self.SSM_blocks[i](input_t, (h_prev, g_prev)) # Pass a single tensor\n",
        "                hidden_state[i] = (h_t, g_prev)\n",
        "                input_t = h_t # Use the output of the current layer as input for the next layer\n",
        "\n",
        "        # Decode the hidden state of the last time step of the last layer\n",
        "        h_prev, g_prev = hidden_state[-1]\n",
        "\n",
        "        out = self.W_C(h_prev) + self.W_D(x[:, seq_len - 1, :])+ self.W_J(g_prev)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MG SMM-s"
      ],
      "metadata": {
        "id": "REFTok4wChEe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oZDpHc6MqhtD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class MgSmmSCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, gate_size):\n",
        "        super(MgSmmSCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gate_size = gate_size\n",
        "\n",
        "        # linear input gate\n",
        "        self.W_A = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_B = nn.Linear(input_size, hidden_size)\n",
        "        self.W_bh = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # multiplicative input gate\n",
        "        self.W_E = nn.Linear(gate_size, gate_size)\n",
        "        self.W_F = nn.Linear(input_size, gate_size)\n",
        "        self.W_bg = nn.Parameter(torch.zeros(gate_size))\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        h_prev, g_prev = hidden_state\n",
        "\n",
        "        # Hidden state\n",
        "        h_t =  self.W_A(h_prev) + self.W_B(x) +  self.W_bh\n",
        "        g_t =  self.W_E(g_prev) * torch.repeat_interleave(x, self.gate_size, dim=1) + self.W_F(x) + self.W_bg\n",
        "\n",
        "        return h_t, g_t\n",
        "\n",
        "class MgSmmSModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, gate_size = 0):\n",
        "        \"\"\"\n",
        "        Initializes the CustomMambaModel.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The number of expected features in the input $x$.\n",
        "            hidden_size (int): The number of features in the hidden state $h$.\n",
        "            num_layers (int): Number of recurrent layers.\n",
        "            output_size (int): The size of the output from the final linear layer.\n",
        "            gate_size (int): The number of features in the multiplicative gate cell.\n",
        "        \"\"\"\n",
        "        super(MgSmmSModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if gate_size == 0:\n",
        "            gate_size = int(hidden_size/2)\n",
        "        self.gate_size = gate_size\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.SSM_blocks = nn.ModuleList([MgSmmSCell(input_size if i == 0 else hidden_size, hidden_size, gate_size) for i in range(num_layers)])\n",
        "\n",
        "        self.W_C = nn.Linear(hidden_size, output_size)\n",
        "        self.W_D = nn.Linear(input_size, output_size)\n",
        "        self.W_J = nn.Linear(gate_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        hidden_state = [(torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
        "                                torch.ones(batch_size, self.gate_size).to(x.device)\n",
        "                         ) for _ in range(self.num_layers)]\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            input_t = x[:, t, :]\n",
        "            for i in range(self.num_layers):\n",
        "                h_prev, g_prev = hidden_state[i]\n",
        "\n",
        "                # Mamba part (using the Custom_MambaCell)\n",
        "                h_t, g_t = self.SSM_blocks[i](input_t, (h_prev, g_prev)) # Pass a single tensor\n",
        "                hidden_state[i] = (h_t, g_t)\n",
        "                input_t = h_t # Use the output of the current layer as input for the next layer\n",
        "\n",
        "        # Decode the hidden state of the last time step of the last layer\n",
        "        h_prev, g_prev = hidden_state[-1]\n",
        "\n",
        "        out = self.W_C(h_prev) + self.W_D(x[:, seq_len - 1, :])+ self.W_J(g_prev)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiOu1vFruRK"
      },
      "source": [
        "#Covid Dataset\n",
        "from https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-epidemiology.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXLB1FxiiOVy",
        "outputId": "df9e8581-40e7-4a93-d845-e5e251b7b18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17OT5EC49yCgBdbG-SPbobIBqBN17BV1v\n",
            "From (redirected): https://drive.google.com/uc?id=17OT5EC49yCgBdbG-SPbobIBqBN17BV1v&confirm=t&uuid=5f9251b2-5c2a-4b59-b7e8-6aece904357b\n",
            "To: /content/epidemiology.csv\n",
            "100% 521M/521M [00:10<00:00, 49.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 17OT5EC49yCgBdbG-SPbobIBqBN17BV1v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7cg7_FE9gBw",
        "outputId": "e4ef01df-c122-4955-8f0a-57c3d3cfd035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_a7Vev2gkCXzn54gN7GD63MUBUb5ZeAD\n",
            "To: /content/alpha2_country.csv\n",
            "\r  0% 0.00/3.87k [00:00<?, ?B/s]\r100% 3.87k/3.87k [00:00<00:00, 10.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# mapping code from https://datahub.io/core/country-list\n",
        "!gdown 1_a7Vev2gkCXzn54gN7GD63MUBUb5ZeAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iRNeyrMNkLbH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('/content/epidemiology.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: thailand_covid_data.csv not found. Please make sure the file is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6V0YYlmo9ypX",
        "outputId": "7fd5b15c-deb4-459c-c2b8-e2a557deabfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Name Code\n",
              "0       Afghanistan   AF\n",
              "1           Albania   AL\n",
              "2           Algeria   DZ\n",
              "3    American Samoa   AS\n",
              "4           Andorra   AD\n",
              "..              ...  ...\n",
              "244  Western Sahara   EH\n",
              "245           Yemen   YE\n",
              "246          Zambia   ZM\n",
              "247        Zimbabwe   ZW\n",
              "248   Åland Islands   AX\n",
              "\n",
              "[249 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-916cdb62-edab-481b-9590-a292a14701c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Albania</td>\n",
              "      <td>AL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>DZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>American Samoa</td>\n",
              "      <td>AS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>Western Sahara</td>\n",
              "      <td>EH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>Yemen</td>\n",
              "      <td>YE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>Zambia</td>\n",
              "      <td>ZM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>Zimbabwe</td>\n",
              "      <td>ZW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>Åland Islands</td>\n",
              "      <td>AX</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>249 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-916cdb62-edab-481b-9590-a292a14701c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-916cdb62-edab-481b-9590-a292a14701c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-916cdb62-edab-481b-9590-a292a14701c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_29a0836a-b4cc-4bda-b4bf-bcd686234824\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('country_code_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_29a0836a-b4cc-4bda-b4bf-bcd686234824 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('country_code_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "country_code_df",
              "summary": "{\n  \"name\": \"country_code_df\",\n  \"rows\": 249,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 249,\n        \"samples\": [\n          \"Malta\",\n          \"Anguilla\",\n          \"Holy See (Vatican City State)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 248,\n        \"samples\": [\n          \"BG\",\n          \"AI\",\n          \"RE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "country_code_df = pd.read_csv('/content/alpha2_country.csv')\n",
        "country_code_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc35qkVkFbTm",
        "outputId": "f2f58155-26c3-414c-a142-e4c2eb4ea5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of DataFrames created for the 40 countries.\n"
          ]
        }
      ],
      "source": [
        "selected_contry_codes = ['US', 'IN', 'BR', 'FR', 'DE',\n",
        "                                      'GB', 'RU', 'IT', 'TR', 'ES',\n",
        "                                      'VN', 'AR', 'AU', 'AT', 'BD',\n",
        "                                      'BE', 'BG', 'CA', 'CL', 'CN',\n",
        "                                      'CU', 'DK', 'FI', 'GE', 'GR',\n",
        "                                      'ID', 'JP', 'JO', 'KE', 'KR',\n",
        "                                      'LR', 'MY','ML', 'MX', 'NL',\n",
        "                                      'NO', 'PH','SE', 'CH', 'TH']\n",
        "selected_country_names = ['USA'   , 'INDIA', 'BRAZIL', 'FRANCE', 'GERMANY',\n",
        "                                        'UK', 'RUSSIA', 'ITALY', 'TURKEY', 'SPAIN',\n",
        "                                        'VIETNAM', 'ARGENTINA', 'AUSTRALIA', 'AUSTRIA', 'BANGLADESH',\n",
        "                                        'BELGIUM', 'BULGARIA', 'CANADA', 'CHILE', 'CHINA',\n",
        "                                        'CUBA', 'DENMARK', 'FINLAND', 'GEORGIA', 'GREECE',\n",
        "                                        'INDONESIA', 'JAPAN', 'JORDAN'  , 'KENYA', 'KOREA',\n",
        "                                        'LIBERIA', 'MALAYSIA', 'MALI', 'MEXICO', 'NETHERLANDS',\n",
        "                                        'NORWAY', 'PHILIPPINES', 'SWEDEN', 'SWITZERLAND', 'THAILAND']\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "start_date = '2020-01-06'\n",
        "end_date = '2022-06-06'\n",
        "\n",
        "df = df[(df['date'] >= start_date) & (df['date'] <= end_date)].copy()\n",
        "\n",
        "clean_df = df.drop(['new_confirmed', 'new_deceased', 'new_recovered', 'new_tested', \"cumulative_deceased\", \"cumulative_recovered\", \"cumulative_tested\"], axis = 1)\n",
        "\n",
        "country_dfs = []\n",
        "\n",
        "for code in selected_contry_codes:\n",
        "    country_df = clean_df[clean_df['location_key'] == code].copy()\n",
        "    country_dfs.append(country_df)\n",
        "\n",
        "print(f\"List of DataFrames created for the {len(selected_contry_codes)} countries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEPyU2liuJ1e"
      },
      "source": [
        "#Dataset prep format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BhusVh1AKJ9c"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, seq_length, pred_idx):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:(i + seq_length)]\n",
        "        y = data[i + seq_length, pred_idx] # Predict only the first column ('cumulative_confirmed')\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jocwjwMFtf4G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "\n",
        "## Covid part\n",
        "dataset_name = \"Covid\"\n",
        "\n",
        "data_list = []\n",
        "\n",
        "for i in range(len(selected_contry_codes)):\n",
        "\n",
        "    # Select the columns for the time series\n",
        "    cumulative_confirmed_data = country_dfs[i]['cumulative_confirmed'].values.astype(float)\n",
        "\n",
        "    # Scale the 'cumulative_confirmed' data\n",
        "    scaler_confirmed = MinMaxScaler(feature_range=(-1, 1))\n",
        "    cumulative_confirmed_scaled = scaler_confirmed.fit_transform(cumulative_confirmed_data.reshape(-1, 1))\n",
        "\n",
        "    ####### dataset part #####\n",
        "    seq_length = 30 # You can adjust this\n",
        "\n",
        "    X, y = create_sequences(cumulative_confirmed_scaled, seq_length,0) # perdict temp 0\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    # Reshape y to have a size of (batch_size, 1)\n",
        "    y = y.unsqueeze(1)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    test_size = len(X) - train_size\n",
        "\n",
        "    X_raw_train, X_raw_test = X[:train_size], X[train_size:]\n",
        "    y_raw_train, y_raw_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    data_list.append((X_raw_train, y_raw_train, X_raw_test, y_raw_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akwHy3spxa-q"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08XYUaXnzBxB"
      },
      "source": [
        "Hyperparameter and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YMMaF5rRy6Gg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set a fixed seed for reproducibility\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wiDYiuZyzFp4"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = X_raw_train.shape[-1] # Updated input size to include 'cumulative_confirmed'\n",
        "hidden_size = 32\n",
        "num_layers = 1\n",
        "output_size = 1\n",
        "num_epochs = 1000 # You can adjust this\n",
        "learning_rate = 0.001\n",
        "\n",
        "num_runs = 1\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_raw_dataset = TensorDataset(X_raw_train, y_raw_train)\n",
        "test_raw_dataset = TensorDataset(X_raw_test, y_raw_test)\n",
        "\n",
        "batch_size = 64 # You can adjust this\n",
        "train_raw_loader = DataLoader(train_raw_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_raw_loader = DataLoader(test_raw_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f6a464"
      },
      "source": [
        "# Experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiv6rXh_kgcL"
      },
      "source": [
        "##Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DYzkPbgtkf1d"
      },
      "outputs": [],
      "source": [
        "def bi_lstm_part(train_data, eval_loader, test_data, num_runs):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  bi_lstm_test_losses = []\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- bi LSTM Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = BidirectionalLSTMModel(input_size, hidden_size, num_layers, output_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = nn.L1Loss()\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Print loss every few epochs\n",
        "          # if (epoch + 1) % 50 == 0:\n",
        "          #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "                # Move data to GPU if available\n",
        "                  sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "                  targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  # print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # print(\"Training finished.\")\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              outputs = model(sequences)\n",
        "              test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          # print(f'Original LSTM Test Loss: {avg_test_loss:.4f}')\n",
        "          bi_lstm_test_losses.append(avg_test_loss)\n",
        "\n",
        "  return bi_lstm_test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK_GNgW_ZYBi"
      },
      "source": [
        "##Original LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "55387060"
      },
      "outputs": [],
      "source": [
        "def OriginalLSTM_part(train_data, eval_loader, test_data, num_runs):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  original_lstm_test_losses = []\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- Original LSTM Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = OriginalLSTMModel(input_size, hidden_size, num_layers, output_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = nn.L1Loss()\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Print loss every few epochs\n",
        "          # if (epoch + 1) % 50 == 0:\n",
        "          #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "                  # Move data to GPU if available\n",
        "                  sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "                  targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  # print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # print(\"Training finished.\")\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              outputs = model(sequences)\n",
        "              test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          # print(f'Original LSTM Test Loss: {avg_test_loss:.4f}')\n",
        "          original_lstm_test_losses.append(avg_test_loss)\n",
        "\n",
        "  return original_lstm_test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnNGtbO5H3mY"
      },
      "source": [
        "##Original GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OVceAEkOH453"
      },
      "outputs": [],
      "source": [
        "def OriginalGRU_part(train_data, eval_loader, test_data, num_runs):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  original_gru_test_losses = []\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- Original GRU Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = UnidirectionalGRUModel(input_size, hidden_size, num_layers, output_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = nn.L1Loss()\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Print loss every few epochs\n",
        "          # if (epoch + 1) % 50 == 0:\n",
        "          #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "                  # Move data to GPU if available\n",
        "                  sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "                  targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  # print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # print(\"Training finished.\")\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              outputs = model(sequences)\n",
        "              test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          # print(f'Original GRU Test Loss: {avg_test_loss:.4f}')\n",
        "          original_gru_test_losses.append(avg_test_loss)\n",
        "\n",
        "  return original_gru_test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEPu9xrl8Ldv"
      },
      "source": [
        "##Custom Mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1SF9-JQG8Lzu"
      },
      "outputs": [],
      "source": [
        "def custom_Mamba_part(train_data, eval_loader, test_data, num_runs, loss_function = nn.L1Loss(), eval_function = nn.MSELoss()):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  custom_Mamba_test_losses = []\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- Custom Mamba Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = Custom_MambaModel(input_size, hidden_size, num_layers, output_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = loss_function\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Print loss every few epochs\n",
        "          # if (epoch + 1) % 50 == 0:\n",
        "          #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "                  # Move data to GPU if available\n",
        "                  sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "                  targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  # print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # print(\"Training finished.\")\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              outputs = model(sequences)\n",
        "              test_loss += eval_function(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          # print(f'Custom mamba Test Loss: {avg_test_loss:.4f}')\n",
        "          custom_Mamba_test_losses.append(avg_test_loss)\n",
        "\n",
        "  return custom_Mamba_test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RTtUvLvqj9c"
      },
      "source": [
        "##MG SMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kRQXXRpeqpCK"
      },
      "outputs": [],
      "source": [
        "def mg_smm_part(train_data, eval_loader, test_data, num_runs, loss_function = nn.L1Loss(), eval_function = nn.MSELoss(), gate_size = 32):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  mg_smm_test_losses = []\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- mg_smm Mamba Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = MgSmmModel(input_size, hidden_size, num_layers, output_size, gate_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = loss_function\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      ## Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Print loss every few epochs\n",
        "          # if (epoch + 1) % 50 == 0:\n",
        "          #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "                # Move data to GPU if available\n",
        "                  sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "                  targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  # print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # print(\"Training finished.\")\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              # Move data to GPU if available\n",
        "              sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              outputs = model(sequences)\n",
        "              test_loss += eval_function(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          print(f'mg_smm mamba Test Loss: {avg_test_loss:.4f}')\n",
        "          mg_smm_test_losses.append(avg_test_loss)\n",
        "\n",
        "  return mg_smm_test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MG SMM-s"
      ],
      "metadata": {
        "id": "YiyrFh8YFJbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oNtXoH3X5m4d"
      },
      "outputs": [],
      "source": [
        "def mg_smm_s_part(train_data, eval_loader, test_data, num_runs, loss_function = nn.L1Loss(), eval_function = nn.MSELoss(), gate_size = 32):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  mg_smm_s_test_losses = []\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- mg_smm_s Mamba Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = MgSmmSModel(input_size, hidden_size, num_layers, output_size, gate_size)\n",
        "      # .to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = loss_function\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Move data to GPU if available\n",
        "              # sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              # targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Print loss every few epochs\n",
        "          # if (epoch + 1) % 50 == 0:\n",
        "          #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "                  # Move data to GPU if available\n",
        "                  # sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "                  # targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  # print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              # Move data to GPU if available\n",
        "              # sequences = sequences.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "              # targets = targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "              outputs = model(sequences)\n",
        "              test_loss += eval_function(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          print(f'mg_smm_s Test Loss: {avg_test_loss:.4f}')\n",
        "          mg_smm_s_test_losses.append(avg_test_loss)\n",
        "  return mg_smm_s_test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff74dbda"
      },
      "source": [
        "# Compare results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GElThJKFJdCl",
        "outputId": "28c4e699-41ab-40d7-8365-9f9d69de6172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "No GPU available\n",
            "FRANCE\n",
            "\n",
            "--- Original LSTM Run 1/3 ---\n",
            "\n",
            "--- Original LSTM Run 2/3 ---\n",
            "\n",
            "--- Original LSTM Run 3/3 ---\n",
            "\n",
            "--- bi LSTM Run 1/3 ---\n",
            "\n",
            "--- bi LSTM Run 2/3 ---\n",
            "\n",
            "--- bi LSTM Run 3/3 ---\n",
            "\n",
            "--- Original GRU Run 1/3 ---\n",
            "\n",
            "--- Original GRU Run 2/3 ---\n",
            "\n",
            "--- Original GRU Run 3/3 ---\n",
            "\n",
            "--- Custom Mamba Run 1/3 ---\n",
            "\n",
            "--- Custom Mamba Run 2/3 ---\n",
            "\n",
            "--- Custom Mamba Run 3/3 ---\n",
            "\n",
            "--- mg_smm Mamba Run 1/3 ---\n",
            "mg_smm mamba Test Loss: 0.0815\n",
            "\n",
            "--- mg_smm Mamba Run 2/3 ---\n",
            "mg_smm mamba Test Loss: 0.0723\n",
            "\n",
            "--- mg_smm Mamba Run 3/3 ---\n",
            "mg_smm mamba Test Loss: 0.2613\n",
            "\n",
            "--- mg_smm_s Mamba Run 1/3 ---\n",
            "mg_smm_s Test Loss: 0.4077\n",
            "\n",
            "--- mg_smm_s Mamba Run 2/3 ---\n",
            "mg_smm_s Test Loss: 1.3407\n",
            "\n",
            "--- mg_smm_s Mamba Run 3/3 ---\n",
            "mg_smm_s Test Loss: 0.6200\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "# from torchmetrics.regression import MeanAbsolutePercentageError\n",
        "from torchmetrics.regression import LogCoshError\n",
        "\n",
        "num_runs = 3\n",
        "start = 3\n",
        "stop = 4\n",
        "\n",
        "All_result = []\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n",
        "\n",
        "for i in range(len(data_list[start:stop])):\n",
        "\n",
        "  idx = start + i\n",
        "  data = data_list[idx]\n",
        "  print(selected_country_names[idx])\n",
        "\n",
        "  X_raw_train, y_raw_train, X_raw_test, y_raw_test = data\n",
        "\n",
        "  eval_size = int(len(X_raw_test) * 0.5)\n",
        "  X_raw_eval, X_raw_test =  X_raw_test[:eval_size], X_raw_test[eval_size:]\n",
        "  y_raw_eval, y_raw_test = y_raw_test[:eval_size], y_raw_test[eval_size:]\n",
        "\n",
        "  # print(\"X__raw_train shape:\", X_raw_train.shape)\n",
        "  # print(\"y__raw_train shape:\", y_raw_train.shape)\n",
        "  country_run = {\"country\":selected_country_names[idx]}\n",
        "\n",
        "  train_data = TensorDataset(X_raw_train, y_raw_train)\n",
        "  eval_data = TensorDataset(X_raw_eval, y_raw_eval)\n",
        "  test_data = TensorDataset(X_raw_test, y_raw_test)\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "  eval_loader = DataLoader(eval_data, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  hidden_size = 256\n",
        "\n",
        "  start_time = time.time()\n",
        "  original_lstm_test_losses = OriginalLSTM_part(train_loader, eval_loader, test_loader, num_runs)\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['original_lstm_test_losses'] = original_lstm_test_losses\n",
        "  country_run['original_lstm_test_losses_time'] = elasped_time\n",
        "\n",
        "  start_time = time.time()\n",
        "  bi_lstm_test_losses = bi_lstm_part(train_loader, eval_loader, test_loader, num_runs)\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['bi_lstm_test_losses'] = bi_lstm_test_losses\n",
        "  country_run['bi_lstm_test_losses_time'] = elasped_time\n",
        "\n",
        "  hidden_size = 128\n",
        "\n",
        "  start_time = time.time()\n",
        "  gru_test_losses = OriginalGRU_part(train_loader, eval_loader, test_loader, num_runs)\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['gru_test_losses'] = gru_test_losses\n",
        "  country_run['gru_test_losses_time'] = elasped_time\n",
        "\n",
        "  hidden_size = 32\n",
        "\n",
        "  start_time = time.time()\n",
        "  custom_Mamba_test_losses = custom_Mamba_part(train_loader, eval_loader, test_loader, num_runs, nn.L1Loss(), nn.MSELoss())\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['custom_Mamba_test_losses'] = custom_Mamba_test_losses\n",
        "  country_run['custom_Mamba_test_losses_time'] = elasped_time\n",
        "\n",
        "  hidden_size = 64\n",
        "\n",
        "  start_time = time.time()\n",
        "  mg_smm_test_losses = mg_smm_part(train_loader, eval_loader, test_loader, num_runs,nn.L1Loss(),nn.MSELoss(), gate_size = 32)\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['mg_smm_test_losses'] = mg_smm_test_losses\n",
        "  country_run['mg_smm_test_losses_time'] = elasped_time\n",
        "\n",
        "  start_time = time.time()\n",
        "  mg_smm_s_test_losses = mg_smm_s_part(train_loader, eval_loader, test_loader, num_runs, nn.MSELoss(),nn.MSELoss(), gate_size = 32)\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['mg_smm_s_test_losses_MSE'] = mg_smm_s_test_losses\n",
        "  country_run['mg_smm_s_test_losses_MSE_time'] = elasped_time\n",
        "\n",
        "  All_result.append(country_run)\n",
        "\n",
        "All_results_df = pd.DataFrame(All_result)#.T\n",
        "All_results_df.to_csv(f'raw_result_{start+1}_{stop}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WYG6gHRrrEn"
      },
      "source": [
        "# Saving model weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SncNu-Z1sy-1"
      },
      "outputs": [],
      "source": [
        "def mg_smm_s_part_saved_model(train_data, eval_loader, test_data, num_runs, loss_function = nn.L1Loss(), eval_function = nn.MSELoss(), gate_size = 32, country_run = \"named\"):  #train_raw_loader, test_raw_loader\n",
        "  # Initialize a list to store test losses\n",
        "  mg_smm_s_test_losses = []\n",
        "  best_test_loss = float('inf')\n",
        "\n",
        "  for run in range(num_runs):\n",
        "      print(f\"\\n--- mg_smm_s Mamba Run {run + 1}/{num_runs} ---\")\n",
        "\n",
        "      # Initialize the model\n",
        "      model = MgSmmSModel(input_size, hidden_size, num_layers, output_size, gate_size)\n",
        "\n",
        "      # Define loss function and optimizer\n",
        "      criterion = loss_function\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Early stopping parameters\n",
        "      best_eval_loss = float('inf')\n",
        "      patience = 50  # Number of epochs to wait for improvement\n",
        "      epochs_no_improve = 0\n",
        "\n",
        "      # Training loop\n",
        "      # print(\"Starting training...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          for i, (sequences, targets) in enumerate(train_data):\n",
        "              # Forward pass\n",
        "              outputs = model(sequences)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              # Backward and optimize\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # Evaluation on the evaluation set\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              eval_loss = 0\n",
        "              for sequences, targets in eval_loader:\n",
        "\n",
        "                  outputs = model(sequences)\n",
        "                  eval_loss += criterion(outputs, targets).item()\n",
        "\n",
        "              avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "          # Early stopping check\n",
        "          if avg_eval_loss < best_eval_loss:\n",
        "              best_eval_loss = avg_eval_loss\n",
        "              epochs_no_improve = 0\n",
        "              # Optionally save the best model state\n",
        "              # torch.save(model.state_dict(), 'best_model.pth')\n",
        "          else:\n",
        "              epochs_no_improve += 1\n",
        "              if epochs_no_improve == patience:\n",
        "                  print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break # Stop training loop\n",
        "\n",
        "      # Evaluation on the test set\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          test_loss = 0\n",
        "          for sequences, targets in test_data:\n",
        "              outputs = model(sequences)\n",
        "              test_loss += eval_function(outputs, targets).item()\n",
        "\n",
        "          avg_test_loss = test_loss / len(test_data)\n",
        "          print(f'mg_smm_s Test Loss: {avg_test_loss:.4f}')\n",
        "          mg_smm_s_test_losses.append(avg_test_loss)\n",
        "\n",
        "          if avg_test_loss < best_test_loss:\n",
        "              best_test_loss = avg_test_loss\n",
        "              print(f\"New best_{country_run}_model\")\n",
        "              torch.save(model.state_dict(), f'best_{country_run}_model.pth')\n",
        "  return mg_smm_s_test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ix9lQgiIFOzS"
      },
      "outputs": [],
      "source": [
        "from os import name\n",
        "country_name_2idx = {}\n",
        "for i in range(len(selected_country_names)):\n",
        "  name = selected_country_names[i]\n",
        "  country_name_2idx[name] = i\n",
        "\n",
        "selected_names = ['USA' ,\n",
        "                            'VIETNAM',\n",
        "                            'JORDAN',\n",
        "                            'KOREA',\n",
        "                            'LIBERIA']\n",
        "idx_for_run = [country_name_2idx[i] for i in selected_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02VMc1ALrqqG",
        "outputId": "637a9d62-bcd8-49a4-bf68-d5204ada7629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "No GPU available\n",
            "USA\n",
            "\n",
            "--- mg_smm_s Mamba Run 1/10 ---\n",
            "Early stopping at epoch 120\n",
            "mg_smm_s Test Loss: 0.0734\n",
            "New best_USA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 2/10 ---\n",
            "Early stopping at epoch 92\n",
            "mg_smm_s Test Loss: 0.0051\n",
            "New best_USA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 3/10 ---\n",
            "Early stopping at epoch 67\n",
            "mg_smm_s Test Loss: 0.0126\n",
            "\n",
            "--- mg_smm_s Mamba Run 4/10 ---\n",
            "Early stopping at epoch 117\n",
            "mg_smm_s Test Loss: 0.0084\n",
            "\n",
            "--- mg_smm_s Mamba Run 5/10 ---\n",
            "Early stopping at epoch 53\n",
            "mg_smm_s Test Loss: 0.0104\n",
            "\n",
            "--- mg_smm_s Mamba Run 6/10 ---\n",
            "Early stopping at epoch 121\n",
            "mg_smm_s Test Loss: 0.0147\n",
            "\n",
            "--- mg_smm_s Mamba Run 7/10 ---\n",
            "Early stopping at epoch 114\n",
            "mg_smm_s Test Loss: 0.0405\n",
            "\n",
            "--- mg_smm_s Mamba Run 8/10 ---\n",
            "Early stopping at epoch 53\n",
            "mg_smm_s Test Loss: 0.3669\n",
            "\n",
            "--- mg_smm_s Mamba Run 9/10 ---\n",
            "Early stopping at epoch 138\n",
            "mg_smm_s Test Loss: 0.0037\n",
            "New best_USA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 10/10 ---\n",
            "Early stopping at epoch 104\n",
            "mg_smm_s Test Loss: 0.0742\n",
            "VIETNAM\n",
            "\n",
            "--- mg_smm_s Mamba Run 1/10 ---\n",
            "Early stopping at epoch 265\n",
            "mg_smm_s Test Loss: 3.5868\n",
            "New best_VIETNAM_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 2/10 ---\n",
            "Early stopping at epoch 105\n",
            "mg_smm_s Test Loss: 0.0001\n",
            "New best_VIETNAM_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 3/10 ---\n",
            "Early stopping at epoch 59\n",
            "mg_smm_s Test Loss: 0.1422\n",
            "\n",
            "--- mg_smm_s Mamba Run 4/10 ---\n",
            "Early stopping at epoch 100\n",
            "mg_smm_s Test Loss: 2.0008\n",
            "\n",
            "--- mg_smm_s Mamba Run 5/10 ---\n",
            "Early stopping at epoch 172\n",
            "mg_smm_s Test Loss: 0.6274\n",
            "\n",
            "--- mg_smm_s Mamba Run 6/10 ---\n",
            "Early stopping at epoch 53\n",
            "mg_smm_s Test Loss: 2.0760\n",
            "\n",
            "--- mg_smm_s Mamba Run 7/10 ---\n",
            "Early stopping at epoch 93\n",
            "mg_smm_s Test Loss: 3.0700\n",
            "\n",
            "--- mg_smm_s Mamba Run 8/10 ---\n",
            "Early stopping at epoch 55\n",
            "mg_smm_s Test Loss: 0.3456\n",
            "\n",
            "--- mg_smm_s Mamba Run 9/10 ---\n",
            "Early stopping at epoch 227\n",
            "mg_smm_s Test Loss: 3.1114\n",
            "\n",
            "--- mg_smm_s Mamba Run 10/10 ---\n",
            "Early stopping at epoch 52\n",
            "mg_smm_s Test Loss: 0.9030\n",
            "JORDAN\n",
            "\n",
            "--- mg_smm_s Mamba Run 1/10 ---\n",
            "Early stopping at epoch 103\n",
            "mg_smm_s Test Loss: 0.0053\n",
            "New best_JORDAN_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 2/10 ---\n",
            "Early stopping at epoch 106\n",
            "mg_smm_s Test Loss: 0.0421\n",
            "\n",
            "--- mg_smm_s Mamba Run 3/10 ---\n",
            "Early stopping at epoch 51\n",
            "mg_smm_s Test Loss: 0.5141\n",
            "\n",
            "--- mg_smm_s Mamba Run 4/10 ---\n",
            "Early stopping at epoch 128\n",
            "mg_smm_s Test Loss: 0.0022\n",
            "New best_JORDAN_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 5/10 ---\n",
            "Early stopping at epoch 81\n",
            "mg_smm_s Test Loss: 0.0378\n",
            "\n",
            "--- mg_smm_s Mamba Run 6/10 ---\n",
            "Early stopping at epoch 73\n",
            "mg_smm_s Test Loss: 0.0327\n",
            "\n",
            "--- mg_smm_s Mamba Run 7/10 ---\n",
            "Early stopping at epoch 54\n",
            "mg_smm_s Test Loss: 0.1149\n",
            "\n",
            "--- mg_smm_s Mamba Run 8/10 ---\n",
            "Early stopping at epoch 133\n",
            "mg_smm_s Test Loss: 0.2555\n",
            "\n",
            "--- mg_smm_s Mamba Run 9/10 ---\n",
            "Early stopping at epoch 156\n",
            "mg_smm_s Test Loss: 0.0193\n",
            "\n",
            "--- mg_smm_s Mamba Run 10/10 ---\n",
            "Early stopping at epoch 57\n",
            "mg_smm_s Test Loss: 0.0594\n",
            "KOREA\n",
            "\n",
            "--- mg_smm_s Mamba Run 1/10 ---\n",
            "Early stopping at epoch 89\n",
            "mg_smm_s Test Loss: 0.0528\n",
            "New best_KOREA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 2/10 ---\n",
            "Early stopping at epoch 58\n",
            "mg_smm_s Test Loss: 0.0181\n",
            "New best_KOREA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 3/10 ---\n",
            "Early stopping at epoch 80\n",
            "mg_smm_s Test Loss: 0.2445\n",
            "\n",
            "--- mg_smm_s Mamba Run 4/10 ---\n",
            "Early stopping at epoch 92\n",
            "mg_smm_s Test Loss: 0.2916\n",
            "\n",
            "--- mg_smm_s Mamba Run 5/10 ---\n",
            "Early stopping at epoch 860\n",
            "mg_smm_s Test Loss: 0.5103\n",
            "\n",
            "--- mg_smm_s Mamba Run 6/10 ---\n",
            "Early stopping at epoch 87\n",
            "mg_smm_s Test Loss: 3.1255\n",
            "\n",
            "--- mg_smm_s Mamba Run 7/10 ---\n",
            "Early stopping at epoch 102\n",
            "mg_smm_s Test Loss: 0.0269\n",
            "\n",
            "--- mg_smm_s Mamba Run 8/10 ---\n",
            "Early stopping at epoch 59\n",
            "mg_smm_s Test Loss: 1.0070\n",
            "\n",
            "--- mg_smm_s Mamba Run 9/10 ---\n",
            "Early stopping at epoch 56\n",
            "mg_smm_s Test Loss: 5.4330\n",
            "\n",
            "--- mg_smm_s Mamba Run 10/10 ---\n",
            "Early stopping at epoch 109\n",
            "mg_smm_s Test Loss: 1.9689\n",
            "LIBERIA\n",
            "\n",
            "--- mg_smm_s Mamba Run 1/10 ---\n",
            "Early stopping at epoch 56\n",
            "mg_smm_s Test Loss: 0.1480\n",
            "New best_LIBERIA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 2/10 ---\n",
            "Early stopping at epoch 84\n",
            "mg_smm_s Test Loss: 0.0092\n",
            "New best_LIBERIA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 3/10 ---\n",
            "Early stopping at epoch 79\n",
            "mg_smm_s Test Loss: 0.0014\n",
            "New best_LIBERIA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 4/10 ---\n",
            "Early stopping at epoch 73\n",
            "mg_smm_s Test Loss: 0.0108\n",
            "\n",
            "--- mg_smm_s Mamba Run 5/10 ---\n",
            "Early stopping at epoch 128\n",
            "mg_smm_s Test Loss: 0.0008\n",
            "New best_LIBERIA_model\n",
            "\n",
            "--- mg_smm_s Mamba Run 6/10 ---\n",
            "Early stopping at epoch 82\n",
            "mg_smm_s Test Loss: 0.0282\n",
            "\n",
            "--- mg_smm_s Mamba Run 7/10 ---\n",
            "Early stopping at epoch 58\n",
            "mg_smm_s Test Loss: 0.0111\n",
            "\n",
            "--- mg_smm_s Mamba Run 8/10 ---\n",
            "Early stopping at epoch 60\n",
            "mg_smm_s Test Loss: 0.0053\n",
            "\n",
            "--- mg_smm_s Mamba Run 9/10 ---\n",
            "Early stopping at epoch 92\n",
            "mg_smm_s Test Loss: 0.0024\n",
            "\n",
            "--- mg_smm_s Mamba Run 10/10 ---\n",
            "Early stopping at epoch 86\n",
            "mg_smm_s Test Loss: 0.0040\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torchmetrics.regression import LogCoshError\n",
        "\n",
        "num_runs = 10\n",
        "start = 20\n",
        "stop = 21\n",
        "\n",
        "All_result = []\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n",
        "\n",
        "for i in idx_for_run:\n",
        "  idx = i\n",
        "  data = data_list[idx]\n",
        "  print(selected_country_names[idx])\n",
        "\n",
        "  X_raw_train, y_raw_train, X_raw_test, y_raw_test = data\n",
        "\n",
        "  eval_size = int(len(X_raw_test) * 0.5)\n",
        "  X_raw_eval, X_raw_test =  X_raw_test[:eval_size], X_raw_test[eval_size:]\n",
        "  y_raw_eval, y_raw_test = y_raw_test[:eval_size], y_raw_test[eval_size:]\n",
        "\n",
        "  country_run = {}\n",
        "\n",
        "  train_data = TensorDataset(X_raw_train, y_raw_train)\n",
        "  eval_data = TensorDataset(X_raw_eval, y_raw_eval)\n",
        "  test_data = TensorDataset(X_raw_test, y_raw_test)\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "  eval_loader = DataLoader(eval_data, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # best param set\n",
        "  hidden_size = 64\n",
        "\n",
        "  start_time = time.time()\n",
        "  mg_smm_s_test_losses = mg_smm_s_part_saved_model(train_loader, eval_loader, test_loader, num_runs, nn.HuberLoss(), nn.MSELoss(), gate_size = 64, country_run= selected_country_names[idx])\n",
        "  end_time = time.time()\n",
        "  elasped_time = end_time - start_time\n",
        "  country_run['mg_smm_s_test_losses_Huber'] = mg_smm_s_test_losses\n",
        "  country_run['mg_smm_s_test_losses_Huber_time'] = elasped_time\n",
        "\n",
        "  All_result.append(country_run)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}